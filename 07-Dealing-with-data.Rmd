
# Dealing with data: dplyr - Week Eight

```{r , echo=FALSE, eval=TRUE, include=TRUE}
klippy::klippy(c('r', 'bash'), position = c('top', 'right'), tooltip_message = 'copy to clipboard', tooltip_success = 'Copied!')
```

```{r include=FALSE}
knitr::opts_chunk$set(eval=FALSE)
```


## Let's get going

You need to accept the latest assignment from [Github Classrooms](https://classroom.github.com/a/ifsL9ZE1).

We will be using this project until the end of term, so make sure you make regular commits, and push your work at the end of each session. 

## Introduction to dplyr

In chapters 3 and 4 we demonstrated a brief data analysis on the `palmerpenguins` dataset. We ran through several pipelines, but without going into huge amounts of detail on each section. 

In this chapter we are going to get really well acquainted with **dplyr** @R-dplyr. 

We will look at several core functions:

* `select` (get some columns)

* `filter` (get some rows)

* `arrange` (order the rows)

* `mutate` (make new columns)

* `group_by` (add grouping information)

* `summarise` (calculate summary information)

You should make careful notes about these functions and what they do. Build scripts with carefully added notes that you can use for future work

As you work through this (and other chapters) - make notes, if you take a break, set up a commit, or reach the end of a session - push your work to Github.

Make sure you are using the [R Cheat Sheets for dplyr](https://www.rstudio.com/resources/cheatsheets/)

## select

Start by setting up the packages you will need for this session

```{r, eval=TRUE}
library(tidyverse)
library(lubridate)

penguins <- read_csv("data/penguins_simple.csv")
```
> ** Note - if you don't have these packages available, then you will need to run `install.packages("tidyverse")` - you should do this in the *console* and **not** your script. 

We use select to *select variables* from a dataframe or tibble. for example:

```
data_set %>% 
  select(variable_1, variable_2)
```
> ** Note this is *pseudocode* an example, not something you should run.

* The data is piped into the first argument position of the `select` function

* We then include the arguments where each one is the name of a variable in the dataset

For example if we actually run this:

```{r}
penguins %>% 
  select(flipper_length_mm)
```

To look at flipper length - we should note a couple of things.

* Variable name is **not** in quotes

* The original data is unchanged e.g. penguins. Unless we assign the results of `select` with the assignment arrow (<-), the results will just be printed in the console

* The order you ask for the variables in `select` determines the order of the columns in any new dataset

* `select` returns a tibble

* If we want to keep most variables, but *remove one*, use the minus operator

```{r}
penguins <- penguins %>% 
  select(-flipper_length_mm)

```

```{block, type="rmdwarning"}
In the above option we just *overwrote* the penguin dataset with a new version that *does not* contain flipper length. 
Be careful - the above code will thrown an error message if you try to run it again (there is no longer a flipper length variable to remove). 
If you want to *undo* this, the easiest way is to re-run your script to just before this line. 

```

You can also use select to keep sets of consecutive variables together

```{r}
penguins %>% 
  select(species:flipper_length_mm)
```

There are also a number of helper functions like `starts_with`, `ends_with`, `contains`, `matches`. So if we want to keep all the variables that start with "b"

```{r}
penguins %>% 
  select(starts_with("b"))

```

## mutate

Adding or creating new variables is a common task, we might want to make a log-transformation, subtract one value from another or use `mutate` to add a new date variable

```{r}
penguins <- penguins %>% 
  mutate(date_proper=dmy(date))

```

Sometimes we want to change the values being used. For example we might want to change "male" and "female" to some abbreviation like "M" and "F". (Probably not, but it's an example!).

```{r}
  penguins %>% 
  mutate(sex=case_when(sex == "male" ~ "M",
                       sex == "female" ~ "F"))

```

Things to know about `mutate`:

* Unless we assign the results of `mutate` back to an object (<-) the changes won't be saved

* We can use newly created variables in further calculations *within* the same `mutate`

## filter

`filter` is used to subset observations in a dataframe. We might want to see the observations where `flipper_length_mm` is larger or smaller than a fixed value. Or we might want to only see the data from female penguins, or a particular species. 

```{r}
penguins %>% 
  filter(sex == "female",
         species == "Adelie",
         flipper_length_mm < 180)

```

In this example we've created a subset of `penguins` that only includes the five observations where flipper length is less than 180mm in female, Adelie penguins. 

We can also set an either/or filter, for example if we only want small & large flipper lengths.

```{r}
penguins %>% 
  filter(species == "Adelie", 
         flipper_length_mm <180 | 
           flipper_length_mm >200)

```

This creates a subset of Adelie penguins, that only includes 14 observations where flipper length is less than 180mm or greater than 200mm. 

The vertical bar | is understood by R as OR. 

The alternative is to look at values *between* two amounts

```{r}
penguins %>% 
  filter(species == "Adelie",
         between(flipper_length_mm, 180,200))

```


## arrange

We use arrange to reorder the rows of our data. Instances where we 'need' to do this are rare. But we may often want to reorder rows so that we can understand the data more easily

```{r}
penguins_ordered <- penguins %>% 
  arrange(sex,body_mass_g)

# arrange data first by sex - all females then all males, within each sex order body mass from low to high
  
penguins_ordered %>% 
  select(penguin_id, sex, body_mass_g)
# view just a few variables

```

By default arrange sorts from low to high (and alphabetically) - we can reverse this if we wrap the variable names in the function `desc`

```{r}
penguins_reverse_ordered <- penguins %>% 
  arrange(desc(sex,body_mass_g))


  
penguins_reverse_ordered %>% 
  select(penguin_id, sex, body_mass_g)


# we can also apply this to specific variables
penguins_reverse_ordered <- penguins %>% 
  arrange(sex,desc(body_mass_g))

```


## Group and summarise

Very often we want to make calculations aobut groups of observations, such as the mean or median. We are often interested in comparing responses among groups. For example, we previously found the number of distinct penguins in our entire dataset

```{r}
penguins %>% 
  summarise(n_distinct(penguin_id))

```

Now consider when the groups are subsets of observations, as when we find out the number of penguins in each species and sex

```{r}
penguins %>% 
  group_by(species, sex) %>% 
  summarise(n_distinct(penguin_id))

```

We are using summarise and group_by a lot! They are very powerful functions:

* `group_by` adds *grouping* information into a data object, so that subsequent calculations happen on a *group-specific* basis. 

* `summarise` is a data aggregation function thart calculates summaries of one or more variables, and it will do this separately for any groups defined by `group_by`

### Using summarise

```{r}
penguins %>% 
  summarise(mean_flipper_length = mean(flipper_length_mm, na.rm=TRUE),
   mean_bill_length = mean(bill_length_mm, na.rm=TRUE))

```
> **Note - we provide informative names for ourselves on the left side of the =. 

There are a number of different calculations we can use including:

* `min` and `max` to calculate minimum and maximum values of a vector

* `mean` and `median` 

* `sd` and `var` calculate standard deviation and variance of a numeric vector

We can use several functions in `summarise`. Which means we can string several calculations together in a single step

```{r}
penguins %>% 
  summarise(n=n(),
            num_penguins = n_distinct(penguin_id),
            mean_flipper_length = mean(flipper_length_mm, na.rm=TRUE),
            prop_female = sum(sex == "female", na.rm=TRUE) / n())
            

```
> **Note - we have placed each argument on a separate line. This is just stylistic, it makes the code easier to read. 


### Summarize all columns

We can use the function `across` to count up the number of NAs in every column (by specifying `everything`). 

```{r}

penguins %>% 
  summarise(across(.cols = everything(), 
                   .fns = ~sum(is.na(.)))) %>% 
  glimpse()

```

It has two arguments, `.cols` and `.fns`. The `.cols` argument lets you specify column types, while the `.fns` argument applies the required function to all of the selected columns. 


```{r}
# the mean of ALL numeric columns in the data, where(is.numeric) hunts for numeric columns

penguins %>% 
  summarise(across(.cols = where(is.numeric), 
                   .fns = ~mean(., na.rm=TRUE)))

```

The above example calculates the means of any & all numeric variables in the dataset. 

The below example is a slightly complicated way of running the n_distinct for summarise. The `.cols()` looks for any column that contains the word "penguin" and runs the `n_distinct()`command of these

```{r}
# number of distinct penguins, as only one column contains the word penguin

penguins %>% 
  summarise(across(.cols = contains("penguin"), 
                   .fns = ~n_distinct(.))) %>% 
  glimpse()

```

### group_by summary

The `group_by` function provides the ability to separate our summary functions according to any subgroups we wish to make. The real magic happens when we pair this with `summarise` and `mutate`.

In this example, by grouping on the individual penguin ids, then summarising by n - we can see how many times each penguin was monitored in the course of this study. 

```{r}
penguin_stats <- penguins %>% 
  group_by(penguin_id) %>% 
  summarise(num=n())
```
> **Note - the actions of group_by are powerful, but group_by on its own doesn't change the visible structure of the dataframe. 

### More than one grouping variable

What if we need to calculate by more than one variable at a time? 

```{r}
penguins_grouped <- penguins %>% 
  group_by(sex, species)

```

 We can then calculate the mean flipper length of penguins in each of the six combinations

```{r}
penguin_summary <- penguins_grouped %>% 
summarise(mean_flipper = mean(flipper_length_mm, na.rm=TRUE))
```

Now the first row of our summary table shows us the mean flipper length (in mm) for female Adelie penguins. There are eight rows in total, six unique combinations and two rows where the sex of the penguins was not recorded(`NA`)

#### using group_by with mutate

When `mutate` is used with a grouped object, the calculations occur by 'group'. Here's an example:

```{r}
centered_penguins <- penguins %>% 
  group_by(sex, species) %>% 
  mutate(flipper_centered = flipper_length_mm-mean(flipper_length_mm, na.rm=TRUE))

```

Here we are calculating a **group centered mean**, this new variable contains the *difference* between each observation and the mean of whichever group that observation is in. 

### remove group_by

On occasion we may need to remove the grouping information from a dataset. This is often required when we string pipes together, when we need to work using a grouping structure, then revert back to the whole dataset again

Look at our grouped dataframe, and we can see the information on groups is at the top of the data:

```
# A tibble: 344 x 10
# Groups:   sex, species [8]
   species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g
   <chr>   <chr>           <dbl>         <dbl>            <dbl>       <dbl>
 1 Adelie  Torge~           39.1          18.7              181        3750
 2 Adelie  Torge~           39.5          17.4              186        3800
 3 Adelie  Torge~           40.3          18                195        3250
 ```


```{r}
centered_penguins %>% 
  ungroup()

```

Look at this output - you can see the information on groups has now been removed from the data. 

# Dealing with data part 2 - Week Nine

```{r , echo=FALSE, eval=TRUE, include=TRUE}
klippy::klippy(c('r', 'bash'), position = c('top', 'right'), tooltip_message = 'copy to clipboard', tooltip_success = 'Copied!')
```

```{r include=FALSE}
knitr::opts_chunk$set(eval=FALSE)
```

## Expanding the data toolkit

In this chapter we go through some more miscellaneous topics around dealing with data.

## Pipes

As you have seen we often link our dplyr verbs together with `pipes`. Using one function after another e.g. `mutate` then `group_by` and perhaps `summarise`
The pipe %>% allows us to make "human-readable" an logical strings of instructions. 

The two below examples of pseudocode would be read identically by R - but which one is easier for you to understand?

```{r, eval=TRUE, echo=FALSE, out.width="80%", fig.alt= "Two very different ways of presenting the same code"}
knitr::include_graphics("images/pipe.png")
```

One way we can try and breakdown the complexity of multiple nested brackets is to make lots of "intermediate" R objects:

```{r}
penguins_grouped <- group_by(penguins, species, sex)

penguins_grouped_full <- drop_na(penguins_grouped, sex)

penguins_flipper_length <- summarise(penguins_grouped_full, mean=mean(flipper_length_mm, na.rm=TRUE))

penguins_flipper_length

```
> **Note - nothing wrong with this, but look at how many R objects are cluttering up your environment tab which you will probably never use again!

Or...

```{r}
penguins %>% 
  group_by(., species, sex) %>% 
  drop_na(., sex) %>% 
  summarise(., mean=mean(flipper_length_mm))

```
> **Note - check for yourself whether the outcome is identical

The pipe is becoming increasingly popular - because it makes code so much more readable. The newest versions of base R (4.1 and above) have even incorporate their own version of a pipe `|>` (https://michaelbarrowman.co.uk/post/the-new-base-pipe/). 

But how does the pipe actually work? Well whenever you include a pipe in your code, it signals to take everything on the left-hand side of the pipe and place it as an "argument" in the code which then comes on the right hand side. The . in the code above shows you the placeholder where the argument is placed. 

When you use the pipe (as you have been doing previously), you don't actually need to include the . for most functions. If you don't put it in, the pipe will simply place it into the *first argument* position. For this reason you will see that all dplyr functions have the first argument as the data. 

```{r}
penguins %>% 
  group_by(species, sex) %>% 
  drop_na(sex) %>% 
  summarise(mean=mean(flipper_length_mm))

```

## Strings

Datasets often contain words, and we call these words "strings". Often these aren't quite how we want them to be, but we can manipulate these as much as we like. Functions in the package `stringr` @R-stringr, are fantastic. And the number of different types of manipulations are endless

```{r}
str_replace_all(names(penguins), c("e"= "E"))
```

### separate

Sometimes a string might contain two pieces of information in one. This does not confirm to our tidy data principles. But we can easily separate the information with `separate` from `tidyr` @R-tidyr

First we produce some made-up data

```{r}
df <- tibble(label=c("a-1", "a-2", "a-3")) 
#make a one column tibble
df
```

```{r}
df %>% 
  separate(label, c("treatment", "replicate"), sep="-")

```

We started with one variable called `label` and then split it into two variables, `treatment` and `replicate`, with the split made where `-` occurs. 
The opposite of this function is `unite()`

### More stringr

Check out (https://stringr.tidyverse.org/index.html) @R-stringr


```{r}
penguins %>% 
  mutate(species=str_to_upper(species))

```

```{r}
penguins %>% 
  mutate(species=str_remove_all(species, "e"))

```

We can also trim leading or trailing empty spaces with `str_trim`. These are often problematic and difficult to spot e.g.

```{r}
df2 <- tibble(label=c("penguin", " penguin", "penguin ")) 
df2
```

We can easily imagine a scenario where data is manually input, and trailing or leading spaces are left in. These are difficult to spot by eye - but problematic because as far as R is concerned these are different values. We can use the function `distinct` to return the names of all the different levels it can find in this dataframe.

```{r}
df2 %>% 
  distinct()
```

If we pipe the data throught the `str_trim` function to remove any gaps, then pipe this on to `distinct` again - by removing the whitespace, R now recognised just one level to this data. 

```{r}
df2 %>% 
  mutate(label=str_trim(label, side="both")) %>% 
  distinct()

```
A quick example of how to extract partial strings according to a pattern is to use `grepl`. Combined with `filter` it is possible to subset dataframe by searching for all the strings that match provided information, such as all the penguin IDs that start with "N1"

```{r}
penguins %>% 
  filter(grepl("N1", penguin_id)) %>% 
  distinct(penguin_id)

```


## Dates and times

Dates and times are difficult. 

There's a lot of different ways to write the same date

13-10-2019

10-13-2019

13-10-19

13th Oct 2019

2019-10-13

This variability makes it difficult to tell our software how to read the information, luckily we can use `lubridate` @R-lubridate. We can tell R the order of our date (or time) data.

Back in the last chapter (\@ref(mutate)), we used `mutate` and `lubridate` to convert date data which had been parsed as a string, into date format with the function `dmy()`. Depending on how we interpret the date ordering in a file, we can use `ymd`, `ydm`, `mdy` etc. 

If you get a warning that some dates could not be parsed, then you might find the date date has been inconsistently entered into the dataset.

```{r}
penguins %>% 
  select(date, date_proper) %>% 
  head()

```

Once we have established our date data, we are able to perform calculations. Such as the date range across which our data was collected. Try this with our variable in string form and it will provide the alphabetical order instead.  

```{r}
penguins %>% 
  summarise(min_date=min(date_proper),
            max_date=max(date_proper))
```

### Calculations with dates

How many times was each penguin measured, and across what total time period?

```{r}
penguins %>% 
  group_by(penguin_id) %>% 
  summarise(min=min(date_proper), 
            max=max(date_proper), 
            difference = max-min, 
            n=n())
```

Cool we can also convert intervals such as days into weeks, months or years with `dweeks(1)`, `dmonths(1)`, `dyears(1)`.

As with all cool functions, you should check out the RStudio cheat sheet for more information. Date type data is common in datasets, and learning to work with it is a uesful skill. 


```{r}
penguins %>% 
  group_by(penguin_id) %>% 
  summarise(min=min(date_egg_proper), 
            max=max(date_egg_proper), 
            difference = (max-min)/dyears(1),
            n=n()) %>% 
  arrange(desc(difference))

```

## Pivot

In the previous chapters we discussed the principles of tidy data (\@ref(tidy-data)), tidy data is easy to analyse, but not always the format you will find data in.

The most likely format you will find data in is a "wide format", where we have columns of related data. Luckily `tidyr` contains a number of **pivot** functions


```{r , echo=FALSE, eval=TRUE}
knitr::include_graphics("images/original-dfs-tidy.png")
```


Let's start with some dummy data - we will make a dataframe of three penguins and the number of three types of prey they are observed to eat over a week:

```{r}

penguin_id <- c("N1A1","N1A2","N2A1")
krill <- c(70,20,43)
fish <- c(5, 19,4)
squid <- c(11,5,0)


df3 <- tibble(penguin_id, krill, fish, squid)
df3
```

This dataset is perfectly legible, but does not confirm to tidy data principles, each row is not a unique observation, but instead contains three observations of types of prey eaten.
Most of the time, we can make data "tidy" but pivoting wide data into a long format. In this example we should rearrange the data to produce a longer dataframe with fewer columns. We should make a new column for prey "type" and a column for "amount"

```{r}
df3_long <- df3 %>% 
  pivot_longer(cols=(krill:squid), names_to = "prey_type", values_to = "consumption_per_week")
df3_long
```

It then becomes extremely easy to plot this data:

```{r}
df3_long %>% 
  ggplot(aes(x=penguin_id, y=consumption_per_week, fill=prey_type))+
  geom_bar(stat="identity", position=position_dodge())

```

Once again, there's an RStudio cheatsheet for this, so check it out!

You may very occasionally need to use `pivot_wider()` to generate tidy data, but it's unusual, the above scenario is the most likely messy data you will encounter!




## Summing up 

### What we learned

You have learned:

The most common data manipulation functions you will need to import and tidy data, and the way in which they operate. 

* How to create new columns as products of functions or calculations

* How to select and filter data to access the subsets you are interested in

* How to apply functions to data groups

* How the pipe works, and why we use it

* How to deal with data subtypes like strings and dates

* How to reshape data with pivot

You have used `tidyverse` and the functions made available from:

* `dplyr` @R-dplyr

* `tidyr` @R-tidyr

You have also used two packages, which are also installed as part of the tidyverse, but have to be called with separate library() functions

* `lubridate` @R-lubridate

* `stringr` @R-stringr


### Further Reading, Guides and tips

[R Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/)


# Deeper data insights part 1 - Week Ten

```{r , echo=FALSE, eval=TRUE, include=TRUE}
klippy::klippy(c('r', 'bash'), position = c('top', 'right'), tooltip_message = 'copy to clipboard', tooltip_success = 'Copied!')
```

```{r include=FALSE}
knitr::opts_chunk$set(eval=FALSE)
```

In these last chapters on generating data insights we are going to cover

* Types of variable

* Exploring numeric variables

* Exploring categorical variables

* Dealing with missing data

Before we dive into this it is important to understand where this data has come from, is it a single study? Is it a lab experiment, fieldwork survey, exploratory work or from a carefully designed study?

You should also play close attention to the data, and remind yourself **frequently** how many variables do you have and what are their names? How many rows/observations do you have?

## Variables

### Numerical

You **should** already be familiar with the concepts of numerical and categorical data. **Numeric** variables have values that describe a measure or quantity. This is also known as quantitative data. We can subdivide numerical data further:

* **Continuous numeric variables.** This is where observations can take any value within a range of numbers. Examples might include body mass (g), age, temperature or flipper length (mm). 
While in theory these values can have any numbers, within a dataset they are likely bounded (set within a minimum/maximum of observed or measurable values), and the accuracy may only be as precise as the measurement protocol allows. 

* **Discrete numeric variables** Observations are numeric but restricted to *whole values* e.g. 1,2,3,4,5 etc. These are also known as **integers**. Discrete variables could include the number of individuals in a population, number of eggs laid etc. Anything where it would make no sense to describe in fractions e.g. a penguin cannot lay 2 and a half eggs. Counting!

### Categorical

Values that describe a characteristic of data such as 'what type' or 'which category'. Categorical variables are mutually exclusive - one observation should not be able to fall into two categories at once - and should be exhaustive - there should not be data which does not *fit* a category (not the same as NA - not recorded). Categorical variables are qualitative, and often represented by non-numeric values such as words. It's a bad idea to represent categorical variables as numbers (R won't treat it correctly). Categorical variables can be defined further as:

* **Ordinal variables** Observations can take values that can be logically ordered or ranked. Examples include - activity levels (sedentary, moderately active, very active); size classes (small, medium, large). 

* **Nominal variables** Observations that can take values that are not logically ordered. Examples include Species or Sex in the Penguins data. 

It is important to order Ordinal variables in the their logical order value when plotting data visuals or tables. Nominal variables are more flexible and could be ordered in whatever pattern works best for your data (perhaps you could order them according to the values of another numeric variable). 

```{block, type="rmdwarning"}
*Don't use numbers to describe categorical information* e.g. (Adelie = 1, Gentoo =2, Chinstrap = 3). This can be done, but it isn't usually very sensible. It's clearer to use the words themselves, and helps when making tables and graphs later.

It's easy to assign levels to categorical data:

Example: 

penguins <- penguins %>% 
mutate(species = factor(species, 
                  levels = c("Adelie",
                  "Gentoo",
                  "Chinstrap")))

```

## Understanding Numerical variables

Let's take a look at some of our variables

```{r, eval=TRUE}
glimpse(penguins)
```

We can see that bill length contains numbers, and that many of these are fractions, but only down to 0.1mm. By comparison body mass all appear to be discrete number variables. Does this make body mass an integer? The underlying quantity (bodyweight) is clearly continuous, it is clearly possible for a penguin to weigh 3330.7g but it might *look* like an integer because of the way it was measured. This illustrates the importance of understanding the the type of variable you are working with - just looking at the values isn't enough. 

On the other hand, how we choose to measure and record data *can* change the way it is presented in a dataset. If the researchers had decided to simply record small, medium and large classes of bodyweight, then we would be dealing with ordinal categorical variables. These distinctions can become less clear if we start to deal with multiple classes of ordinal categories - for example if the researchers were measuring body mass to the nearest 10g. It might be reasonable to treat these as integers...

### Graphing a numeric variable

```{r}

adelie_penguins <- penguins %>% 
  filter(species=="Adelie")

adelie_summary <- adelie_penguins %>% 
  summarise(mean=mean(body_mass_g, 
                      na.rm=TRUE))

ggplot()+
  geom_histogram(data= adelie_penguins,
                 aes(x=body_mass_g),
                 bins=10)+ #remember it is a good idea to try multiple bins of data
  geom_vline(data=adelie_summary,
             aes(xintercept=mean),
             colour="red",
             linetype="dashed")

```

```{r, eval=TRUE, echo=FALSE}

adelie_penguins <- penguins %>% 
  filter(species=="Adelie")

adelie_summary <- adelie_penguins %>% 
  summarise(mean=mean(body_mass_g, 
                      na.rm=TRUE),
            median=median(body_mass_g,
                          na.rm=TRUE))

ggplot()+
  geom_histogram(data= adelie_penguins,
                 aes(x=body_mass_g),
                 bins=10)+ #remember it is a good idea to try multiple bins of data
  geom_vline(data=adelie_summary,
             aes(xintercept=mean),
             colour="red",
             linetype="dashed")

```

Using this distribution, we can see that the data appears to fit a normal/gaussian distribution - mean body mass is slightly under 3750g. Penguins smaller than 3000g are rare. 

### Insights about body mass

The histogram gives us a nice summary of the sample distribution of the body_mass_g variable. It reveals (1) the most common values, (2) the range of values and (3) the shape of the distribution. Remember it's a very good idea to play with the number of bins in order to see whether this changes the shape of the histogram. 

Let's construct the histogram again with 30 bins. As well we will make some other tweaks. 

```{r}
ggplot()+
  geom_histogram(data=adelie_penguins,
                 aes(x=body_mass_g),
                 bins=50, # fifty bins
                 fill="steelblue",
                 colour="darkgrey",
                 alpha=0.8)+
  labs(x="Body mass (g) of Adelie penguins",
       y = "Count")

```

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
ggplot()+
  geom_histogram(data=adelie_penguins,
                 aes(x=body_mass_g),
                 bins=50, # fifty bins
                 fill="steelblue",
                 colour="darkgrey",
                 alpha=0.8)+
  labs(x="Body mass (g) of Adelie penguins",
       y = "Count")

```

Changing the colours and labels is purely aesthetic, and not needed *at all* for data exploration, but look how pretty! 

What is important is that increasing the number of bins indicates that their *might* be two peaks in our data. We should think carefully about the other variables (probably categorical) that could be used to subset our data to investigate this. This requires careful thought about our data - in this instance - it seems sensible to try and see whether splitting the data by sex accounts for this


```{r}
adelie_penguins %>% 
  drop_na(sex) %>% 
ggplot()+
  geom_histogram(aes(x=body_mass_g,
                     fill=sex),
                 bins=50, # fifty bins
                 colour="darkgrey",
                 alpha=0.8,
                 position="identity")+
  labs(x="Body mass (g) of Adelie penguins",
       y = "Count")

```

Let's finish this section by looking at density. This is sensible to use when we have "large datasets", and also allows us to make comparisons between groups with different sample sizes. 

```{r}
adelie_penguins %>% 
  drop_na(sex) %>% 
ggplot()+
  geom_density(aes(x=body_mass_g,
                     fill=sex),
                 colour="darkgrey",
                 alpha=0.8,
                 position="identity")+
  labs(x="Body mass (g) of Adelie penguins",
       y = "Count")
```

Or we can use one of my favourite packages `ggridges` @R-ggridges  which let's us separate out different groups along the y-axis.

```{r}
adelie_penguins %>% 
  drop_na(sex) %>% 
ggplot()+
  ggridges::geom_density_ridges(aes(x=body_mass_g,
                                    y=sex),
                                    alpha=0.8)
```
```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
adelie_penguins %>% 
  drop_na(sex) %>% 
ggplot()+
  ggridges::geom_density_ridges(aes(x=body_mass_g,
                                    y=sex),
                                    alpha=0.8)
```

## Descriptive statistics

We have, so far, been spending our time describing the properties of data by examining graphs. Now we can start to build accurate and specific terms to the descriptions of our data.  

* **central tendency** describes the typical (central) value of a distribution. The most well known description of central tendency is the arithmetic mean, however you should be comfortable with the idea that the median may be a better representation of the central tendency for some data distributions

* **dispersion** describes how a distribution is spread out. Dispersion measures the variability or scatter of a variable. If one distribution is more dispersed than another, this means that in some sense it encompasses a wider range of values. Basic statistics courses often tend to focus on variance adn the standard deviation as two ways to measure/sumamrise dispersion. However, the interquartile range is another method often used in exploratory analysis. 

In the next section we will use the median and interquartile ranges as effective measures of central tendency and dispersion - we will use graphics for this - and more specifically look at boxplots. 

### Central tendency

We can find both the mean and median easily with the summarise function.

```{r, eval=TRUE}
penguin_body_mass_summary <- penguins %>% 
  summarise(mean_body_mass=mean(body_mass_g, na.rm=T),
            median_body_mass=median(body_mass_g, na.rm=T))

penguin_body_mass_summary

```


```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
penguins %>% 
ggplot()+
  geom_density(aes(x=body_mass_g),
               alpha=0.8)+
   geom_vline(data=penguin_body_mass_summary,
             aes(xintercept=mean_body_mass),
             colour="red",
             linetype="dashed")+
     geom_vline(data=penguin_body_mass_summary,
             aes(xintercept=median_body_mass),
             colour="black",
             linetype="dashed")
```


If we do this for the entire penguins dataset - we can clearly see that the mean value has bee significantly "right-shifted" by the long tail of the data distribution. However,this is much less apparent for the median. In this way we can say that the median is less sensitive to the distribution of the data than the mean is.

### Dispersion

Dispersion (how spread out the data is) is an important component towards understanding any numeric variable. Important measures for statistics are **variance** and **standard deviation**. These are both non-negative, smaller values indicate observations tend to be similar in value, while high values indicate these observations are more spread out. We can quickly calculate these with `var` and `sd` - but they probably aren't the best place to *start* exploring your data from. 

Variance is not an intuitive measure - calculated from squaring the deviation of each data point from the sample mean - it gives no overall insight into the shape of the distribution, and it is on a different scale to the original measurements. Standard deviation is the square root of the variance. This means it is on the same scale as the observation, making it easier to interpret, but just like variance it doesn't provide much insight into the overall shape of the distribution, and like the mean can be affected by outliers. 

When trying to understand data it can be simpler and easier to use a measure that does not suffer from these issues outlined above. Instead we can use the **interquartile range**. 

The interquartile range (IQR) is the range that contains the "middle 50%" of our data sample. This is given as the difference between the third and first quartiles. The reason we like to use the IQR is that the more spread out the data is, the larger the IQR. It will also indicate the shape of the distribution and is less affected by outliers than variance. 


We can use the IQR function to find the interquartile range of the body mass variable

```{r}
penguins %>% 
  summarise(IQR_body_mass = IQR(body_mass_g, na.rm=TRUE))
```

The IQR is also useful when applied to the summary plots 'box and whisker plots'. We can also calculate the values of the IQR margins, and add labels with `scales` @R-scales. 

```{r}
penguins %>%
  summarise(q_body_mass = quantile(body_mass_g, c(0.25, 0.5, 0.75), na.rm=TRUE),
            quantile = scales::percent(c(0.25, 0.5, 0.75))) # scales package allows easy converting from data values to perceptual properties

```

```
# A tibble: 3 x 2
  q_body_mass quantile
        <dbl> <chr>   
1        3550 25%     
2        4050 50%     
3        4750 75%    
```

We can see for ourselves the IQR is obtained by subtracting the body mass at tht 75% quantile from the 25% quantile (4750-3550 = 1200).

### Visualising dispersion

```{r}
penguins %>% 
  ggplot()+
  geom_boxplot(aes(x="",
                   y= body_mass_g),
               fill="darkorange",
               colour="steelblue",
               width=0.4)+
  labs(x= "Bodyweight",
       y = "Mass (g)")+
  theme_minimal()

```
```{r, echo=FALSE, eval=TRUE, fig.cap = "A boxplot of the body mass variable showing the medan and IQR", warning=FALSE, message=FALSE}
penguins %>% 
  ggplot()+
  geom_boxplot(aes(x="",
                   y= body_mass_g),
               fill="darkorange",
               colour="steelblue",
               width=0.4)+
  labs(x= "Bodyweight",
       y = "Mass (g)")+
  theme_minimal()

```

> **Note - we forced ggplot2 to hide the tick mark label on the x axis by coding a dummy label with x = " "

We now have several compact representations of the body_mass_g including a histogram, boxplot and summary calculations. You can *and should* generate the same summaries for your other numeric variables. These tables and graphs provide the detail you need to understand the central tendency and dispersion of numeric variables. 

### Combining histograms and boxplots

```{r}

library(patchwork) # put this at the TOP of your script

penguins_na_sex <- penguins %>% 
  drop_na(sex)

colours <-  c("darkorange", "cyan") # set colour scheme here to save on repeating code
lims <- c(3000,6000) # set axis limits here to save on repeating code

  p1 <- ggplot(data = penguins_na_sex,
    aes(x = species,
             y = body_mass_g,
             fill = sex))+
      geom_boxplot()+
    scale_fill_manual(values = colours)+
    scale_y_continuous(limits=lims)+
    labs(x="",
         y="")+
        coord_flip()+ # rotate box plot 90 degrees
    theme_minimal()+
    theme(legend.position="none")

  p2 <- ggplot(data = penguins_na_sex,
         aes(x = body_mass_g,
             y = species,
             fill = sex))+
    ggridges::stat_density_ridges(quantile_lines = TRUE)+
        scale_fill_manual(values = colours)+
    scale_x_continuous(limits=lims)+
    labs(y="",
         x = "Body Mass (g)")+
    theme_minimal()
  
  (p1/p2) # patchwork command to layer one plot above the other

```


```{r, eval=TRUE, echo=FALSE, fig.cap = "This figure shows the use of patchwork to combine two ggplots, demonstrating the boxplot and geom density figures show the same distributions of the data", warning=FALSE, message=FALSE}

library(patchwork) # put this at the TOP of your script

penguins_na_sex <- penguins %>% 
  drop_na(sex)

colours <-  c("darkorange", "cyan")
lims <- c(3000,6000)

  p1 <- ggplot(data = penguins_na_sex,
    aes(x = species,
             y = body_mass_g,
             fill = sex))+
      geom_boxplot()+
    scale_fill_manual(values = colours)+
    scale_y_continuous(limits=lims)+
    labs(x="",
         y="")+
        coord_flip()+
    theme_minimal()+
    theme(legend.position="none")

  p2 <- ggplot(data = penguins_na_sex,
         aes(x = body_mass_g,
             y = species,
             fill = sex))+
    ggridges::stat_density_ridges(quantile_lines = TRUE)+
        scale_fill_manual(values = colours)+
    scale_x_continuous(limits=lims)+
    labs(y="",
         x = "Body Mass (g)")+
    theme_minimal()
  
  (p1/p2)
```

### Missing values

We first met `NA` back in (\@ref(missing-values---na)) and you will hopefully have noticed, either here or in those previous chapters, that missing values `NA` can really mess up our calculations. There are a few different ways we can deal with missing data:

* `drop_na()` on everything before we start. This runs the risk that we lose **a lot** of data as *every* row, with an NA in *any column* will be removed

* `drop_na()` on a particular variable. This is fine, but we should approach this cautiously - if we do this in a way where we write this data into a new object e.g. `penguins <- penguins %>% drop_na(body_mass_g)` then we have removed this data forever - perhaps we only want to drop those rows for a specific calculation - again they might contain useful information in other variables. 

* `drop_na()` for a specific task - this is a more cautious approach **but** we need to be aware of another phenomena. Is the data **missing at random**? You might need to investigate *where* your missing values are in a dataset. Data that is truly **missing at random** can be removed from a dataset without introducing bias. However, if bad weather conditions meant that researchers could not get to a particular island to measure one set of penguins that data is **missing not at random** this should be treated with caution. If that island contained one particular species of penguin, it might mean we have complete data for only two out of three penguin species. There is nothing you can do about incomplete data other than be aware that data not missing at random could influence your distributions. 


## Categorical variables

Ok that was a lot of information - well done - have some praise.

```{r}
praise::praise()
# have some praise
```

Now let's look at categorical variables  -remember these can be **ordinal** or **nominal**. We don't have anything we would classify as ordinal data in the penguins dataset.

We can look at species with the function `distinct`:

```{r}
penguins %>% 
  distinct(species)

```

```
# A tibble: 3 x 1
  species  
  <chr>    
1 Adelie   
2 Gentoo   
3 Chinstrap
```
 We can clearly see there would be no sense to applying an order to these three categories, we can order them in whatever way suits best when presenting our data. 
 
 
### Summaries


```{r}
penguins %>% 
  count(species, sort=TRUE)

```
```
# A tibble: 3 x 2
  species       n
  <chr>     <int>
1 Adelie      152
2 Gentoo      124
3 Chinstrap    68
```

It might be useful for us to make some quick data summaries here

```{r, eval=TRUE}
prob_obs_species <- penguins %>% 
  count(species, sort=TRUE) %>% 
  mutate(prob_obs = n/sum(n))

prob_obs_species

```
So about 44% of our sample is made up of observations from Adelie penguins. When it comes to making summaries about categorical data, that's about the best we can do, we can make observations about the most common categorical observations, and the relative proportions. 

```{r}
penguins %>% 
  ggplot()+
  geom_bar(aes(x=species))
```

```{r, echo=FALSE, eval=TRUE}
penguins %>% 
  ggplot()+
  geom_bar(aes(x=species))
```

This chart is ok - but can we make anything better?

We could go for a stacked bar approach

```{r}
penguins %>% 
  ggplot(aes(x="",
             fill=species))+ # specify fill = species to ensure colours are defined by species
  geom_bar(position="fill")+ # specify fill forces geom_bar to calculate percentages
  scale_y_continuous(labels=scales::percent)+ #use scales package to turn y axis into percentages easily
  labs(x="",
       y="")+
  theme_minimal()

```

```{r, echo=FALSE, eval=TRUE}
penguins %>% 
  ggplot(aes(x="",
             fill=species))+ # specify fill = species to ensure colours are defined by species
  geom_bar(position="fill")+ # specify fill forces geom_bar to calculate percentages
  scale_y_continuous(labels=scales::percent)+ #use scales package to turn y axis into percentages easily
  labs(x="",
       y="")+
  theme_minimal()

```

This graph is OK but not great, the height of each section of the bar represents the relative proportions of each species in the dataset, but this type of chart becomes increasingly difficult to read as more categories are included. Colours become increasingly samey,and it is difficult to read where on the y-axis a category starts and stops, you then have to do some subtraction to work out the values. 

The best graph is then probably the first one we made - with a few minor tweak we can rapidly improve this. 

```{r}
penguins %>% 
  mutate(species=factor(species, levels=c("Adelie",
                                          "Gentoo",
                                          "Chinstrap"))) %>% # set as factor and provide levels
  ggplot()+
  geom_bar(aes(x=species),
           fill="steelblue",
           width=0.8)+
  labs(x="Species",
       y = "Number of observations")+
  geom_text(data=prob_obs_species,
            aes(y=(n+10),
                x=species,
                label=scales::percent(prob_obs)))+
  coord_flip()+
  theme_minimal()

```

```{r, echo=FALSE, eval=TRUE}
penguins %>% 
  mutate(species=factor(species, levels=c("Adelie",
                                          "Gentoo",
                                          "Chinstrap"))) %>% # set as factor and provide levels
  ggplot()+
  geom_bar(aes(x=species),
           fill="steelblue",
           width=0.8)+
  labs(x="Species",
       y = "Number of observations")+
  geom_text(data=prob_obs_species,
            aes(y=(n+10),
                x=species,
                label=scales::percent(prob_obs)))+
  coord_flip()+
  theme_minimal()

```


## Summing up

In this chapter we have really focused on single variables, understanding variable types and their distributions. We learned 

* About different types of data

* How to estimate central tendencies

* Dispersions of numeric and categorical variables

* How to visualise metrics

You have primarily used `tidyverse` packages, but also: 

* `ggridges` @R-ggridges

* `scales` @R-scales

In the next chapter we will look at generating insights around the relationships between variables. Now is a good time to review earlier chapters such as (\@ref(workflow-part-one---week-two)), and think about how we ask questions of our data. 



### Reward

Haven't you done well???

Enjoy this stupid reward from @R-ggcats

```{r, eval=TRUE}
library(Ecdat)
data(incomeInequality)

library(ggcats)
library(gganimate)

 dat <-
   incomeInequality %>%
   select(Year, P99, median) %>%
   rename(income_median = median,
          income_99percent = P99) %>%
   pivot_longer(cols = starts_with("income"),
                names_to = "income",
                names_prefix = "income_")

dat$cat <- rep(NA, 132)

dat$cat[which(dat$income == "median")] <- "nyancat"
dat$cat[which(dat$income == "99percent")] <- rep(c("pop_close", "pop"), 33)

ggplot(dat, aes(x = Year, y = value, group = income, color = income)) +
   geom_line(size = 2) +
   ggtitle("ggcats, a core package of the memeverse") +
   geom_cat(aes(cat = cat), size = 5) +
   xlab("Cats") +
   ylab("Cats") +
   theme(legend.position = "none",
         plot.title = element_text(size = 20),
         axis.text = element_blank(),
         axis.ticks = element_blank()) +
   transition_reveal(Year)

```
